{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/bgramaje/.local/lib/python3.10/site-packages (0.27.6)\n",
      "Requirement already satisfied: aiohttp in /home/bgramaje/.local/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in /home/bgramaje/.local/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.20 in /home/bgramaje/.local/lib/python3.10/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bgramaje/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bgramaje/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bgramaje/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bgramaje/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bgramaje/.local/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bgramaje/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bgramaje/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/bgramaje/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bgramaje/.local/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/bgramaje/.local/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ast import literal_eval\n",
    "from dotenv import load_dotenv, find_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# import environment variable into notebook\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')  # get openai api key\n",
    "print(API_KEY)\n",
    "openai.api_key = API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_request(prompt, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    \"\"\" \n",
    "    Helper function to make a request to openai api, and return it's response.\n",
    "    It acts as a teacher providing prompts for an student gpt-3.5-turbo model\n",
    "    args:\n",
    "        prompt (string)\n",
    "        model (str)\n",
    "        temperature (int)\n",
    "    returns:\n",
    "        str  \n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"      \n",
    "            You're going to act like a teacher. \n",
    "            Your main task will be to teach another instance \\\n",
    "            of the model, which will act as your student. \n",
    "\n",
    "            Therefore, you must provide me with examples of \\\n",
    "            the concept I am going to teach you.\n",
    "            The concept that I am going to provide you is \\\n",
    "            going to be delimited by 3 backticks.\n",
    "            \n",
    "            Format your response as a array of  JSON objects with \\\n",
    "            \"role\", \"content\",  as the keys.\n",
    "            \n",
    "            The order of objects would be something like:\n",
    "            \n",
    "            - Example1, Answer of Example1, Example2, Answer of Example2...\n",
    "            \n",
    "            Make the answer for each example provided as short as possible.\n",
    "            \n",
    "            Where in the JSON objects that the content is the example, the role is user \\\n",
    "            and for the JSON objects where the answer is the content, the role is assistant.\n",
    "            \n",
    "            ```{prompt}```\n",
    "            \n",
    "            Return the response as an string without the 3 backticks\n",
    "        \"\"\"\n",
    "    }]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,  # this is the degree of randomness of the model's output\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_request(prompts=[], question=None, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    \"\"\" \n",
    "    Helper function to make a request to openai api, and return it's response.\n",
    "    It acts as an student, with the provided answer from the teacher model\n",
    "    args:\n",
    "        prompts (array)\n",
    "        model (str)\n",
    "        temperature (int)\n",
    "    returns:\n",
    "        str  \n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"      \n",
    "            You're going to act like a student.\n",
    "            \n",
    "            Your main task will be to learn a new concept provided \\\n",
    "            by another instance of the model that acts as a teacher.\n",
    "            \n",
    "            Yo will be given a series of prompts that will be used \\\n",
    "            by you as a context, each prompt with its corresponding \\\n",
    "            role (user | system) and its content.\n",
    "                \n",
    "            Therefore, you will be asked a question and you will \\ \n",
    "            need to answer them with the context provided. The \\\n",
    "            question is located at the final prompt provided as a context to you\n",
    "\n",
    "            Format your response as a JSON object with \\\n",
    "            \"role\", \"content\",  as the keys. The role will \\\n",
    "            always \"assistant\" since you are giving me the answers\n",
    "             \n",
    "            Make the answer for each example provided as short as possible \\\n",
    "            and try to be concise as possible.\n",
    "            \n",
    "            I want you to only reply with the answer with no introduction or conclusion.\n",
    "            \n",
    "            Be aware that the answer that you provide needs to pass some test cases, \\\n",
    "            so please the answer should be the shortest possible, with no extra information \\\n",
    "            and no reasoning or explanation.\n",
    "        \"\"\"\n",
    "    }]\n",
    "\n",
    "    context = messages + prompts\n",
    "    if (question is not None):\n",
    "        context = context + question\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=context,\n",
    "        temperature=temperature,  # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-gpt-3.5-turbo in organization org-qh6IQFxL0fOgxkwg22VsvDPm on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mYou are given a series of features about chains of cardinal directions. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mYou will be given several examples, either positive or negative, and you will \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mpossible values as an answer\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m response \u001b[39m=\u001b[39m teacher_request(prompt\u001b[39m=\u001b[39;49m[{\u001b[39m'\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m: prompt}])\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[teacher]: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39m# print(f\"[student]: question : 'East North East is positive or negative?\")\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# student_response = student_request(\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m#     prompts=literal_eval(response),\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m#     question=[{'role': 'user', 'content': 'East North East is positive or negative?'}]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# print(f\"[student]: {student_response}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 42\u001b[0m, in \u001b[0;36mteacher_request\u001b[0;34m(prompt, model, temperature)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mHelper function to make a request to openai api, and return it's response.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mIt acts as a teacher providing prompts for an student gpt-3.5-turbo model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m    str  \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m messages \u001b[39m=\u001b[39m [{\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m      \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     40\u001b[0m }]\n\u001b[0;32m---> 42\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     43\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     44\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     45\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,  \u001b[39m# this is the degree of randomness of the model's output\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-qh6IQFxL0fOgxkwg22VsvDPm on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "You are given a series of features about chains of cardinal directions. \\\n",
    "You will be given several examples, either positive or negative, and you will \\\n",
    "have to guess a simple concept that we call \"CONCEPT 1\".\n",
    "\n",
    "If you are given:\n",
    "\n",
    "- \"East North East South East North\" and \"East North East South East \\\n",
    "North East North East South East\" are positive examples of \"PATTERN 1\"\n",
    "\n",
    "- \"West\" and \"East South\" as negative examples of \"PATTERN 1\"\n",
    "\n",
    "How can you define \"PATTERN 1\". Try to be as simple and \\\n",
    "concise as possible. It is very important that you verify \\\n",
    "that your answer is correct.\n",
    "\n",
    "The given answer for each question should be only a string. \\\n",
    "It can be only 'positive' or 'negative'. There are no more \\ \n",
    "possible values as an answer\n",
    "\"\"\"\n",
    "response = teacher_request(prompt=[{'role': 'user', 'content': prompt}])\n",
    "print(f\"[teacher]: {response}\")\n",
    "\n",
    "# print(f\"[student]: question : 'East North East is positive or negative?\")\n",
    "# student_response = student_request(\n",
    "#     prompts=literal_eval(response),\n",
    "#     question=[{'role': 'user', 'content': 'East North East is positive or negative?'}]\n",
    "# )\n",
    "# print(f\"[student]: {student_response}\")\n",
    "\n",
    "print(f\"[student]: question > East North East\")\n",
    "student_response = student_request(\n",
    "    prompts=literal_eval(response),\n",
    "    question=[{'role': 'user', 'content': 'East North East'}]\n",
    ")\n",
    "print(f\"[student]: answer > {student_response}\")\n",
    "\n",
    "# print(f\"[student]: question : 'East South East North\")\n",
    "# student_response = student_request(\n",
    "#     prompts=literal_eval(response),\n",
    "#     question=[{'role': 'user', 'content': 'East South East North'}]\n",
    "# )\n",
    "# print(f\"[student]: {student_response}\")\n",
    "\n",
    "print(f\"[student]: question > West South West North\")\n",
    "student_response = student_request(\n",
    "    prompts=literal_eval(response),\n",
    "    question=[{'role': 'user', 'content': 'West South West North'}]\n",
    ")\n",
    "print(f\"[student]: answer > {student_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
